{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5: Advanced Modeling & Optimization\n",
        "\n",
        "**Objective:** Train multiple regression models, tune hyperparameters,\n",
        "compare via cross-validation, and select the best model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from src.models import train_model, tune_model, save_model\n",
        "from src.evaluation import evaluate_model, print_metrics, cross_validate_model, build_comparison_table\n",
        "from src.visualization import plot_actual_vs_predicted, plot_residuals\n",
        "\n",
        "X_train = pd.read_csv(\"../data/processed/X_train.csv\")\n",
        "X_test  = pd.read_csv(\"../data/processed/X_test.csv\")\n",
        "y_train = pd.read_csv(\"../data/processed/y_train.csv\").squeeze()\n",
        "y_test  = pd.read_csv(\"../data/processed/y_test.csv\").squeeze()\n",
        "\n",
        "print(f\"Train: {X_train.shape}  |  Test: {X_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Train Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_names = [\"linear_regression\", \"ridge\", \"lasso\", \"decision_tree\", \"random_forest\"]\n",
        "\n",
        "trained_models = {}\n",
        "results = {}\n",
        "\n",
        "for name in model_names:\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    model = train_model(name, X_train, y_train)\n",
        "    metrics = evaluate_model(model, X_test, y_test)\n",
        "    print_metrics(metrics)\n",
        "    trained_models[name] = model\n",
        "    results[name] = metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: XGBoost (advanced)\n",
        "try:\n",
        "    xgb_model = train_model(\"xgboost\", X_train, y_train)\n",
        "    xgb_metrics = evaluate_model(xgb_model, X_test, y_test)\n",
        "    print_metrics(xgb_metrics)\n",
        "    trained_models[\"xgboost\"] = xgb_model\n",
        "    results[\"xgboost\"] = xgb_metrics\n",
        "except ImportError:\n",
        "    print(\"XGBoost not installed — skipping.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Initial Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "comparison_df = build_comparison_table(results)\n",
        "comparison_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.4 Hyperparameter Tuning\n",
        "\n",
        "Tune the top 2 performing models using RandomizedSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tune Random Forest\n",
        "print(\"Tuning Random Forest...\")\n",
        "best_rf = tune_model(\"random_forest\", X_train, y_train, cv=5, n_iter=20)\n",
        "rf_tuned_metrics = evaluate_model(best_rf, X_test, y_test)\n",
        "print(\"\\nTuned Random Forest:\")\n",
        "print_metrics(rf_tuned_metrics)\n",
        "trained_models[\"random_forest_tuned\"] = best_rf\n",
        "results[\"random_forest_tuned\"] = rf_tuned_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Tune XGBoost (if available)\n",
        "if \"xgboost\" in trained_models:\n",
        "    print(\"Tuning XGBoost...\")\n",
        "    best_xgb = tune_model(\"xgboost\", X_train, y_train, cv=5, n_iter=20)\n",
        "    xgb_tuned_metrics = evaluate_model(best_xgb, X_test, y_test)\n",
        "    print(\"\\nTuned XGBoost:\")\n",
        "    print_metrics(xgb_tuned_metrics)\n",
        "    trained_models[\"xgboost_tuned\"] = best_xgb\n",
        "    results[\"xgboost_tuned\"] = xgb_tuned_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5 Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Cross-validation scores (R²):\\n\")\n",
        "cv_results = {}\n",
        "for name in [\"ridge\", \"random_forest\"]:\n",
        "    print(f\"{name}:\")\n",
        "    cv_results[name] = cross_validate_model(trained_models[name], X_train, y_train, cv=5)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.6 Final Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_comparison = build_comparison_table(results)\n",
        "print(\"\\n=== Final Model Comparison ===\")\n",
        "final_comparison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.7 Regularization & Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "\n",
        "ridge_train_scores, ridge_test_scores = [], []\n",
        "lasso_train_scores, lasso_test_scores = [], []\n",
        "\n",
        "for alpha in alphas:\n",
        "    # Ridge\n",
        "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "    ridge_train_scores.append(r2_score(y_train, ridge.predict(X_train)))\n",
        "    ridge_test_scores.append(r2_score(y_test, ridge.predict(X_test)))\n",
        "    # Lasso\n",
        "    lasso = Lasso(alpha=alpha, max_iter=10000).fit(X_train, y_train)\n",
        "    lasso_train_scores.append(r2_score(y_train, lasso.predict(X_train)))\n",
        "    lasso_test_scores.append(r2_score(y_test, lasso.predict(X_test)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].semilogx(alphas, ridge_train_scores, \"o-\", label=\"Train\")\n",
        "axes[0].semilogx(alphas, ridge_test_scores, \"s--\", label=\"Test\")\n",
        "axes[0].set_title(\"Ridge: Bias-Variance Tradeoff\")\n",
        "axes[0].set_xlabel(\"Alpha (regularization strength)\")\n",
        "axes[0].set_ylabel(\"R² Score\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].semilogx(alphas, lasso_train_scores, \"o-\", label=\"Train\")\n",
        "axes[1].semilogx(alphas, lasso_test_scores, \"s--\", label=\"Test\")\n",
        "axes[1].set_title(\"Lasso: Bias-Variance Tradeoff\")\n",
        "axes[1].set_xlabel(\"Alpha (regularization strength)\")\n",
        "axes[1].set_ylabel(\"R² Score\")\n",
        "axes[1].legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"../reports/figures/bias_variance_tradeoff.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.8 Select & Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Choose the best model based on the comparison table\n",
        "best_name = final_comparison.iloc[0][\"Model\"]\n",
        "print(f\"Best model: {best_name}\")\n",
        "\n",
        "# Look up from trained_models which stores both base and tuned versions\n",
        "best_model = trained_models.get(best_name)\n",
        "if best_model is None:\n",
        "    raise ValueError(\n",
        "        f\"Model '{best_name}' not found in trained_models. \"\n",
        "        f\"Available: {list(trained_models.keys())}\"\n",
        "    )\n",
        "\n",
        "save_model(best_model, \"../models/best_model.joblib\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Phase 5 Summary\n",
        "\n",
        "### Model Comparison (Initial Run — with data leakage)\n",
        "\n",
        "| Model | R² | MAE (BDT) | RMSE (BDT) |\n",
        "|---|---|---|---|\n",
        "| Random Forest | 1.0000 | 47.64 | 471.51 |\n",
        "| Random Forest (tuned) | 1.0000 | 48.12 | 502.63 |\n",
        "| Decision Tree | 0.9999 | 74.02 | 718.02 |\n",
        "| XGBoost | 0.9997 | 394.00 | 1,482.20 |\n",
        "| XGBoost (tuned) | 0.9996 | 313.53 | 1,554.41 |\n",
        "| Linear Regression | 0.9969 | 1,703.62 | 4,554.05 |\n",
        "| Ridge | 0.9969 | 1,705.40 | 4,554.06 |\n",
        "| Lasso | 0.9969 | 1,702.12 | 4,556.44 |\n",
        "\n",
        "**Best model selected:** Random Forest (R² = 1.0, MAE = 47.64 BDT).\n",
        "\n",
        "### Analysis\n",
        "\n",
        "- **Data leakage dominates results.** The near-perfect scores across all models confirm that `Base Fare` and `Tax & Surcharge` leak the target variable. Tree models achieve R²=1.0 because they can perfectly reconstruct the additive relationship via leaf splits.\n",
        "- **Regularization findings:** Ridge and Lasso bias-variance plots show flat R² across all alpha values (~0.997), indicating the linear relationship is so strong that regularization has no meaningful effect.\n",
        "- **Tuning impact:** Hyperparameter tuning provides negligible improvement — the tuned Random Forest actually has slightly worse RMSE (502 vs 472 BDT), consistent with overfitting noise when the signal is trivially learnable.\n",
        "\n",
        "### Concern & Fix Applied\n",
        "\n",
        "The leaking columns (`Base Fare`, `Tax & Surcharge`) have been dropped from the preprocessing pipeline in `src/pipeline.py`. Re-running the pipeline will produce honest metrics reflecting genuine fare prediction difficulty."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}